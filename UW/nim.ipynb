{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import statistics\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Nim():\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = tuple([int((x + 1)) for x in range(0, n_rows * 2, 2)])\n",
    "        self.possible_values_in_rows = []\n",
    "\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx+1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (tuple(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = copy.deepcopy(self.initial_state)\n",
    "        # self.transition_probs = dict()\n",
    "\n",
    "        self.win_states = list()\n",
    "        for idx in range(n_rows):\n",
    "            list_of_zeros = [0] * n_rows\n",
    "            list_of_zeros[idx] = 1\n",
    "            self.win_states.append(tuple(list_of_zeros))\n",
    "        self.win_states = tuple(self.win_states)\n",
    "\n",
    "    # def fill_transition_probs(self):\n",
    "    #     for state in self.states:\n",
    "    #         self.transition_probs[state] = dict()\n",
    "    #         actions = self.get_possible_actions(state)\n",
    "    #         for action in actions:\n",
    "    #             self.transition_probs[state][action] = dict()\n",
    "    #             new_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "    #             self.transition_probs[state][action][new_state] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        # return self.transition_probs[state][action]\n",
    "        next_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "        return next_state\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def nim_sum(self, state):\n",
    "        binary_rows = [format(row, 'b') for row in state]\n",
    "        max_len = max([len(row) for row in binary_rows])\n",
    "        binary_rows = ['0' * (max_len - len(row)) + row for row in binary_rows]\n",
    "\n",
    "        result = ''\n",
    "        for idx in range(max_len):\n",
    "            res = 0\n",
    "            for row in binary_rows:\n",
    "                res += int(row[idx])\n",
    "                res %= 2\n",
    "            result += str(res)\n",
    "        return result\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = -5\n",
    "\n",
    "        # if not int(self.nim_sum(next_state)):\n",
    "        #     reward += 20\n",
    "\n",
    "        # if self.is_terminal(next_state):\n",
    "        #     reward = -15\n",
    "\n",
    "        if next_state in self.win_states:\n",
    "            reward += 100\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)])\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "               self.is_terminal(self.current_state), None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random winrate: 49.68%\n"
     ]
    }
   ],
   "source": [
    "# play at random\n",
    "nim = Nim()\n",
    "# nim.fill_transition_probs()\n",
    "\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        random_action = random.choice(nim.get_possible_actions(nim.current_state))\n",
    "        nim.step(random_action)\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            if turn % 2:\n",
    "                player_1_wins += 1\n",
    "            else:\n",
    "                player_2_wins += 1\n",
    "\n",
    "        turn += 1\n",
    "\n",
    "print(f'Random winrate: {player_1_wins * 100 / (player_1_wins + player_2_wins)}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UCZENIE PASYWNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def value_iteration(nim, gamma, theta):\n",
    "    V = dict()\n",
    "    # nim.fill_transition_probs()\n",
    "\n",
    "    for state in nim.get_all_states():\n",
    "        V[state] = 0\n",
    "\n",
    "    policy = dict()\n",
    "    for current_state in nim.get_all_states():\n",
    "        try:\n",
    "            policy[current_state] = nim.get_possible_actions(current_state)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    while True:\n",
    "        last_mean_value = statistics.fmean(V.values())\n",
    "        for current_state in nim.get_all_states():\n",
    "            actions = nim.get_possible_actions(current_state)\n",
    "            state_action_values = dict()\n",
    "\n",
    "            for action in actions:\n",
    "\n",
    "                state_action_values[action] = 0\n",
    "                next_state = nim.get_next_states(current_state, action)\n",
    "                state_action_values[action] += nim.get_reward(current_state, action, next_state) + gamma * V[next_state]\n",
    "\n",
    "            V[current_state] = max(list(state_action_values.values()))\n",
    "        if abs(statistics.fmean(V.values()) - last_mean_value) < theta:\n",
    "            break\n",
    "\n",
    "    for current_state in nim.get_all_states():\n",
    "\n",
    "        state_action_values = dict()\n",
    "        actions = nim.get_possible_actions(current_state)\n",
    "\n",
    "        for action in actions:\n",
    "            state_action_values[action] = 0\n",
    "            next_state = nim.get_next_states(current_state, action)\n",
    "            state_action_values[action] += (nim.get_reward(current_state, action, next_state) + gamma * V[next_state])\n",
    "        # print(f'{current_state} -------- {state_action_values}')\n",
    "        max_value_action = max(state_action_values, key=state_action_values.get)\n",
    "\n",
    "        if policy[current_state] != max_value_action:\n",
    "            policy[current_state] = max_value_action\n",
    "\n",
    "    return policy, V"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "nim = Nim()\n",
    "optimal_policy, optimal_value = value_iteration(nim, 0.9, 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# optimal_policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm winrate: 85.1%\n"
     ]
    }
   ],
   "source": [
    "# play (player 1) value iteration vs random\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        if not turn % 2:\n",
    "            action = optimal_policy[tuple(nim.current_state)]\n",
    "        else:\n",
    "            action = random.choice(nim.get_possible_actions(nim.current_state))\n",
    "        nim.step(action)\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            if turn % 2:\n",
    "                player_1_wins += 1\n",
    "            else:\n",
    "                player_2_wins += 1\n",
    "        turn += 1\n",
    "\n",
    "print(f'Algorithm winrate: {player_1_wins * 100 / (player_1_wins + player_2_wins)}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UCZENIE AKTYWNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return max([self.get_qvalue(state, action) for action in possible_actions])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        value = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * (reward + gamma * self.get_value(next_state))\n",
    "        self.set_qvalue(state, action, value)\n",
    "\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        possible_actions_dict = dict()\n",
    "\n",
    "        for action in possible_actions:\n",
    "            possible_actions_dict[action] = self.get_qvalue(state, action)\n",
    "\n",
    "        sorted_dict = sorted(possible_actions_dict.items(), key=lambda kv: kv[1])\n",
    "\n",
    "        return random.choice([k for k, v in possible_actions_dict.items() if v == sorted_dict[-1][-1]])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        return self.get_best_action(state)\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        \"\"\"\n",
    "        Function turns off agent learning.\n",
    "        \"\"\"\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def play_and_train_ql(env, agent, player=0):\n",
    "    \"\"\"\n",
    "    This function should\n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    turn = 0 if not player else 1\n",
    "\n",
    "    while not done:\n",
    "        if not turn % 2:\n",
    "            # get agent to pick action given state state.\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        else:\n",
    "            action = random.choice(env.get_possible_actions(state))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "        if done:\n",
    "            turn += 1\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class Nim():\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = tuple([int((x + 1)) for x in range(0, n_rows * 2, 2)])\n",
    "        self.possible_values_in_rows = []\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx+1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (tuple(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = copy.deepcopy(self.initial_state)\n",
    "        self.win_states = list()\n",
    "        for idx in range(n_rows):\n",
    "            list_of_zeros = [0] * n_rows\n",
    "            list_of_zeros[idx] = 1\n",
    "            self.win_states.append(tuple(list_of_zeros))\n",
    "        self.win_states = tuple(self.win_states)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        next_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "        return next_state\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def nim_sum(self, state):\n",
    "        binary_rows = [format(row, 'b') for row in state]\n",
    "        max_len = max([len(row) for row in binary_rows])\n",
    "        binary_rows = ['0' * (max_len - len(row)) + row for row in binary_rows]\n",
    "\n",
    "        result = ''\n",
    "        for idx in range(max_len):\n",
    "            res = 0\n",
    "            for row in binary_rows:\n",
    "                res += int(row[idx])\n",
    "                res %= 2\n",
    "            result += str(res)\n",
    "        return result\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = -5\n",
    "\n",
    "        if self.is_terminal(next_state):\n",
    "            reward = -15\n",
    "\n",
    "        if next_state in self.win_states:\n",
    "            reward = 10\n",
    "\n",
    "        # if not int(self.nim_sum(next_state)):\n",
    "        #     reward += 5\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)])\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "               self.is_terminal(self.current_state), None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "nim = Nim()\n",
    "\n",
    "agent_ql_first = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "                       get_legal_actions=nim.get_possible_actions)\n",
    "\n",
    "agent_ql_second = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "                       get_legal_actions=nim.get_possible_actions)\n",
    "\n",
    "for i in range(10000):\n",
    "    play_and_train_ql(nim, agent_ql_first, 0)\n",
    "    play_and_train_ql(nim, agent_ql_second, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm winrate: 79.39%\n"
     ]
    }
   ],
   "source": [
    "# play ql vs random\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        if not turn % 2:\n",
    "            action = agent_ql_first.get_best_action(nim.current_state)\n",
    "        else:\n",
    "            action = random.choice(nim.get_possible_actions(nim.current_state))\n",
    "\n",
    "\n",
    "        nim.step(action)\n",
    "\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            if turn % 2:\n",
    "                player_1_wins += 1\n",
    "            else:\n",
    "                player_2_wins += 1\n",
    "\n",
    "        turn += 1\n",
    "\n",
    "print(f'Algorithm winrate: {player_1_wins * 100 / (player_1_wins + player_2_wins)}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm winrate: 80.24%\n"
     ]
    }
   ],
   "source": [
    "# play algorithm vs algorithm\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        if not turn % 2:\n",
    "            action = agent_ql_first.get_best_action(nim.current_state)\n",
    "        else:\n",
    "            action = agent_ql_second.get_best_action(nim.current_state)\n",
    "\n",
    "        nim.step(action)\n",
    "\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            if turn % 2:\n",
    "                player_1_wins += 1\n",
    "            else:\n",
    "                player_2_wins += 1\n",
    "\n",
    "        turn += 1\n",
    "\n",
    "print(f'Algorithm winrate: {player_1_wins * 100 / (player_1_wins + player_2_wins)}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MCTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Nim():\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = tuple([int((x + 1)) for x in range(0, n_rows * 2, 2)])\n",
    "        self.possible_values_in_rows = []\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx+1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (tuple(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = copy.deepcopy(self.initial_state)\n",
    "        self.win_states = list()\n",
    "        for idx in range(n_rows):\n",
    "            list_of_zeros = [0] * n_rows\n",
    "            list_of_zeros[idx] = 1\n",
    "            self.win_states.append(tuple(list_of_zeros))\n",
    "        self.win_states = tuple(self.win_states)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        # return self.transition_probs[state][action]\n",
    "        next_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "        return next_state\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = -5\n",
    "\n",
    "        if self.is_terminal(next_state):\n",
    "            reward = -15\n",
    "\n",
    "        if next_state in self.win_states:\n",
    "            reward = 10\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)])\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "               self.is_terminal(self.current_state), None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, parent=None, parent_action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.children = []\n",
    "        self._number_of_visits = 0\n",
    "        self._results = defaultdict(int)\n",
    "        self._results[1] = 0\n",
    "        self._results[-1] = 0\n",
    "        self._untried_actions = None\n",
    "        self.win_states = list()\n",
    "        for idx in range(len(self.state)):\n",
    "            list_of_zeros = [0] * len(self.state)\n",
    "            list_of_zeros[idx] = 1\n",
    "            self.win_states.append(tuple(list_of_zeros))\n",
    "        self.win_states = tuple(self.win_states)\n",
    "\n",
    "    def untried_actions(self):\n",
    "        self._untried_actions = self.get_legal_actions()\n",
    "        print(self._untried_actions)\n",
    "        return self._untried_actions\n",
    "\n",
    "    def q(self):\n",
    "        wins = self._results[1]\n",
    "        loses = self._results[-1]\n",
    "        return wins - loses\n",
    "\n",
    "    def n(self):\n",
    "        return self._number_of_visits\n",
    "\n",
    "    def expand(self):\n",
    "\n",
    "        action = self._untried_actions.pop()\n",
    "        next_state = self.state.move(action)\n",
    "        child_node = MonteCarloTreeSearchNode(\n",
    "            next_state, parent=self, parent_action=action)\n",
    "\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def is_terminal_node(self):\n",
    "        return self.is_game_over()\n",
    "\n",
    "    def rollout(self):\n",
    "        current_rollout_state = self.state\n",
    "\n",
    "        while not current_rollout_state.is_game_over():\n",
    "\n",
    "            possible_moves = current_rollout_state.get_legal_actions()\n",
    "\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state = current_rollout_state.move(action)\n",
    "        return current_rollout_state.game_result()\n",
    "\n",
    "    def backpropagate(self, result):\n",
    "        self._number_of_visits += 1.\n",
    "        self._results[result] += 1.\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(result)\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self._untried_actions) == 0\n",
    "\n",
    "    def best_child(self, c_param=0.1):\n",
    "\n",
    "        choices_weights = [(c.q() / c.n()) + c_param * np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def rollout_policy(self, possible_moves):\n",
    "\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "\n",
    "    def _tree_policy(self):\n",
    "\n",
    "        current_node = self\n",
    "        while not current_node.is_terminal_node():\n",
    "\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n",
    "\n",
    "    def best_action(self):\n",
    "        simulation_no = 100\n",
    "\n",
    "        for i in range(simulation_no):\n",
    "\n",
    "            v = self._tree_policy()\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "\n",
    "        return self.best_child(c_param=0.)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_game_over():\n",
    "            possible_actions.append(self.state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(self.state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(self.state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def is_game_over(self):\n",
    "        if not any(self.state): return True\n",
    "        return False\n",
    "\n",
    "    def game_result(self):\n",
    "        if self.is_game_over():\n",
    "            return -1\n",
    "\n",
    "        elif self.state in self.win_states:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def move(self, action):\n",
    "        return tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(self.state, action)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m root \u001B[38;5;241m=\u001B[39m MonteCarloTreeSearchNode(state\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m7\u001B[39m))\n\u001B[1;32m----> 2\u001B[0m selected_node \u001B[38;5;241m=\u001B[39m \u001B[43mroot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m selected_node\n",
      "Cell \u001B[1;32mIn [19], line 92\u001B[0m, in \u001B[0;36mMonteCarloTreeSearchNode.best_action\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     88\u001B[0m simulation_no \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(simulation_no):\n\u001B[1;32m---> 92\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tree_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     93\u001B[0m     reward \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mrollout()\n\u001B[0;32m     94\u001B[0m     v\u001B[38;5;241m.\u001B[39mbackpropagate(reward)\n",
      "Cell \u001B[1;32mIn [19], line 81\u001B[0m, in \u001B[0;36mMonteCarloTreeSearchNode._tree_policy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     78\u001B[0m current_node \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m current_node\u001B[38;5;241m.\u001B[39mis_terminal_node():\n\u001B[1;32m---> 81\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mcurrent_node\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_fully_expanded\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     82\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m current_node\u001B[38;5;241m.\u001B[39mexpand()\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn [19], line 65\u001B[0m, in \u001B[0;36mMonteCarloTreeSearchNode.is_fully_expanded\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_fully_expanded\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_untried_actions\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "root = MonteCarloTreeSearchNode(state=(1,3,5,7))\n",
    "selected_node = root.best_action()\n",
    "selected_node"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}