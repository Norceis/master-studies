{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:39:46.999895100Z",
     "start_time": "2023-09-10T17:39:43.180897200Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:39:47.015897Z",
     "start_time": "2023-09-10T17:39:47.001896800Z"
    }
   },
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:39:47.137896200Z",
     "start_time": "2023-09-10T17:39:47.018906500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):    \n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:01.677695900Z",
     "start_time": "2023-09-10T17:39:47.078897700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed01101a285e4ff9bbe61eea6b3d5fc2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "# finetuned = \"mmosiolek/polpaca-lora-7b\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    offload_folder=\"offload\"\n",
    ")\n",
    "\n",
    "# model = PeftModel.from_pretrained(model, finetuned).to(\"cuda\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:01.690695700Z",
     "start_time": "2023-09-10T17:40:01.653699200Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:02.241697400Z",
     "start_time": "2023-09-10T17:40:01.666696400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 2124 files [00:00, 3868.85 files/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data_processing import load_raw_fairytales_dataset\n",
    "\n",
    "raw_fairytales = load_raw_fairytales_dataset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['Dawno, dawno temu, w małej wiosce zwanej Doliną Radosnych Mieszkańców, żył pewien dzielny chłopiec o imieniu Bolek',\n 'Był on mały, ale pełen odwagi i fantazji',\n 'Każdego dnia Bolek wyruszał na poszukiwanie przygód, które czekały na niego za żywopłotem, na granicy wioski',\n 'Pewnego pięknego poranka, Bolek postanowił zwiedzić pobliski las',\n 'Spakował swoje ukochane kanapki z masłem orzechowym, napełnił butelkę wodą i ruszył w drogę',\n 'Kiedy dotarł do lasu, zobaczył tajemniczą ścieżkę, którą jeszcze żaden mieszkaniec Doliny nie śmiał podążać',\n 'Oczywiście, Bolek postanowił, że to jest dokładnie to, czego potrzebuje! Wraz ze swoim małym plecakiem na plecach, Bolek wkroczył na nieznane terytorium',\n 'Las był pełen szemrzących drzew i śpiewających ptaków',\n 'Bolek stał zachwycony, kiedy zebrał kilka pięknych kolorowych kwiatów, które później zaniesie swojej mamie',\n 'Wszystko wydawało się urokliwe, aż do momentu, kiedy usłyszał głośne trzaskanie gałęzi',\n 'Bolek podążył za źródłem dźwięku i odkrył, że to nie była zwykła gałązka, ale mała, potłuczona skrzydła Syreny Imelii',\n 'Syrena leżała pokonana na ziemi i wydawała smutne dźwięki',\n 'Bolek nie mógł przejść obojętnie obok takiej potrzebującej duszy',\n 'Niezwłocznie Bolek wyciągnął swoją małą apteczkę z plecaka i delikatnie opatrzył skrzydełka Syreny Imelii',\n 'Syrena była tak wdzięczna za pomoc, że obiecała Bolkowi jedną życzącą gałkę, która spełni dla niego największe marzenie',\n 'Bolek był zachwycony i wpatrywał się w czarujące oczy Syreny, zastanawiając się jakie marzenie spełnić',\n 'Po chwili namysłu, Bolek z uśmiechem na twarzy wyszeptał na ucho Syreny życzenie',\n 'Potem pożegnał się i ruszył dalej starym traktem wypatrując kolejnej przygody',\n 'Wspaniałe lasy ukazały mu swoje zachwycające tajemnice – spotkał magiczne stworzenia, jak wróżki i krasnoludki i odkrył ukryty skarb',\n 'Po kilku godzinach wspaniałych przygód, Bolek zrozumiał, że zaczyna już zmierzchać',\n 'Postanowił zawrócić do wioski, aby podzielić się swoimi opowieściami z mieszkańcami Doliny Radosnych Mieszkańców',\n 'Kiedy wracał, zatrzymał się na chwilę, aby przypomnieć sobie swoje marzenie i zobaczyć, czy się spełniło',\n 'Nagle, pojawiła się Syrena Imelia, unosząc się nad wioską w promieniach zachodzącego słońca',\n 'Miała w ręce piękny, kolorowy balon, który przypominał Bolkowi o jego najbardziej ukochanym odcinku ulubionego programu telewizyjnego',\n 'Był spełnieniem jego marzenia – mógł lecieć ponad Doliną Radosnych Mieszkańców na swym magicznym balonie! Bolek podziękował Syrenie Imelii z całego serca i wspiął się na balon, rozdając uśmiechy i machając mieszkańcom wsi, którzy oglądali go ze zdumieniem',\n 'I tak, świat stał się ogromnym placem zabaw dla małego Bolka, który od tego czasu, razem ze swoim magicznym balonem, docierał do najodleglejszych zakątków świata, przynosząc ze sobą uśmiechy i radość wszędzie tam, gdzie się pojawiał',\n 'A Dolina Radosnych Mieszkańców, gdzie wszystko się zaczęło, była najszczęśliwszym miejscem na ziemi, dzięki chłopcu, który spełnił jedno życzenie i odmienił losy wszystkich mieszkańców na zawsze.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_less_fairytales = []\n",
    "for tale in raw_fairytales:\n",
    "    tale = tale.replace('\\n\\n', ' ')\n",
    "    tale = tale.replace('\\n', ' ')\n",
    "    n_less_fairytales.append(tale)\n",
    "n_less_fairytales[0].split('. ')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:02.282694700Z",
     "start_time": "2023-09-10T17:40:02.242701100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:02.299696700Z",
     "start_time": "2023-09-10T17:40:02.255699800Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "for tale in n_less_fairytales:\n",
    "    tale = tale.split('. ')\n",
    "    for sentence in tale:\n",
    "        dataset_list.append({\"instruction\": \"Napisz bajkę\",\n",
    "                             \"input\": '',\n",
    "                             \"output\": sentence\n",
    "                             })\n",
    "\n",
    "dataset = Dataset.from_list(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:04.706382500Z",
     "start_time": "2023-09-10T17:40:02.287700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3123 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d9a4f8fba334cc9bf2bf50ec8ae8141"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/200 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a5003dc01d94e3abfabb99529cee2e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val = dataset.train_test_split(\n",
    "    test_size=200, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:04.721387600Z",
     "start_time": "2023-09-10T17:40:04.708389800Z"
    }
   },
   "outputs": [],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 300\n",
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:04.941382300Z",
     "start_time": "2023-09-10T17:40:04.723385800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "# model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:04.986380600Z",
     "start_time": "2023-09-10T17:40:04.944382700Z"
    }
   },
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:05.001381400Z",
     "start_time": "2023-09-10T17:40:04.959386300Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T17:40:05.015388200Z",
     "start_time": "2023-09-10T17:40:04.991392500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-10T17:40:05.007387300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/300 : < :, Epoch 0.01/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "# model = torch.compile(model)\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "set(model.hf_device_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "devices = [device for device in set(model.hf_device_map.values()) if device not in [\"cpu\", \"disk\"]]\n",
    "if len(devices) > 1:\n",
    "    print(f\"is_model_parallel: True\")\n",
    "else:\n",
    "    print(f\"is_model_parallel:self.args.device!= {torch.device(devices[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310_alpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
