{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Nim:\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = tuple([int((x + 1)) for x in range(0, n_rows * 2, 2)])\n",
    "        self.possible_values_in_rows = []\n",
    "\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx + 1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (tuple(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = copy.deepcopy(self.initial_state)\n",
    "\n",
    "        self.win_states = list()\n",
    "        for idx in range(n_rows):\n",
    "            list_of_zeros = [0] * n_rows\n",
    "            list_of_zeros[idx] = 1\n",
    "            self.win_states.append(tuple(list_of_zeros))\n",
    "        self.win_states = tuple(self.win_states)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        # return self.transition_probs[state][action]\n",
    "        next_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "        return next_state\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if next_state in self.win_states:\n",
    "            reward += 10\n",
    "\n",
    "        elif self.is_terminal(next_state):\n",
    "            reward += -10\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)])\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "            self.is_terminal(self.current_state), None\n",
    "\n",
    "\n",
    "class WAN(object):  #Weight Agnostic Neural\n",
    "    def __init__(self, init_shared_weight):\n",
    "        self.input_size = 4\n",
    "        self.aVec = np.random.randint(0, 10, 20)\n",
    "        self.wKey = np.random.randint(1, 400, 50)\n",
    "        self.weights = np.random.normal(0, 1, 50)\n",
    "        self.weight_bias = -1.5\n",
    "        nNodes = len(self.aVec)\n",
    "        self.wVec = [0] * (nNodes * nNodes)\n",
    "        self.set_weight(init_shared_weight, 0)\n",
    "\n",
    "    def set_weight(self, weight, weight_bias):\n",
    "        nValues = len(self.wKey)\n",
    "        if isinstance(weight, (list, np.ndarray)):\n",
    "            weights = weight\n",
    "        else:\n",
    "            weights = [weight] * nValues\n",
    "\n",
    "        for i in range(nValues):\n",
    "            k = self.wKey[i]\n",
    "            self.wVec[k] = weights[i] + weight_bias\n",
    "\n",
    "    def tune_weights(self):\n",
    "        self.set_weight(self.weights, self.weight_bias)\n",
    "\n",
    "    def mutate(self, winrate):\n",
    "        mut_chance = np.exp(-6 * winrate + 3)\n",
    "\n",
    "        for gate_idx in range(len(self.aVec)):\n",
    "            if np.random.rand() < mut_chance * 0.05:\n",
    "                new_gate = np.random.randint(0, 10)\n",
    "                while self.aVec[gate_idx] == new_gate:\n",
    "                    new_gate = np.random.randint(0, 10)\n",
    "                self.aVec[gate_idx] = new_gate\n",
    "\n",
    "        for connection_idx in range(len(self.wKey)):\n",
    "            if np.random.rand() < mut_chance * 0.01:\n",
    "                new_connection = np.random.randint(1, 400)\n",
    "                self.wKey[connection_idx] = new_connection\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if state == (0, 0, 0, 0):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_action_by_idx(self, old_state, indices):\n",
    "        move_dict = {0: (1, 0, 0, 0),\n",
    "                     1: (0, 1, 0, 0),\n",
    "                     2: (0, 2, 0, 0),\n",
    "                     3: (0, 3, 0, 0),\n",
    "                     4: (0, 0, 1, 0),\n",
    "                     5: (0, 0, 2, 0),\n",
    "                     6: (0, 0, 3, 0),\n",
    "                     7: (0, 0, 4, 0),\n",
    "                     8: (0, 0, 5, 0),\n",
    "                     9: (0, 0, 0, 1),\n",
    "                     10: (0, 0, 0, 2),\n",
    "                     11: (0, 0, 0, 3),\n",
    "                     12: (0, 0, 0, 4),\n",
    "                     13: (0, 0, 0, 5),\n",
    "                     14: (0, 0, 0, 6),\n",
    "                     15: (0, 0, 0, 7)}\n",
    "\n",
    "        possible_actions = self.get_possible_actions(old_state)\n",
    "        if (0, 0, 0, 0) == possible_actions[0]:\n",
    "            return tuple[0, 0, 0, 0]\n",
    "\n",
    "        for idx in indices:\n",
    "            if move_dict[idx] in possible_actions:\n",
    "                return move_dict[idx]\n",
    "\n",
    "    def get_action(self, old_state):\n",
    "\n",
    "        nNodes = len(self.aVec)\n",
    "        wMat = np.array(self.wVec).reshape((nNodes, nNodes))\n",
    "        nodeAct = [0] * nNodes\n",
    "\n",
    "        for i in range(len(old_state)):\n",
    "            nodeAct[i] = old_state[i]\n",
    "\n",
    "        for iNode in range(self.input_size, nNodes):\n",
    "            rawAct = np.dot(nodeAct, wMat[:, iNode:iNode + 1])\n",
    "            rawAct = self.activate(self.aVec[iNode], rawAct.tolist()[0])\n",
    "            nodeAct[iNode] = rawAct\n",
    "\n",
    "        sorted_indices = np.argsort(-1 * np.array(nodeAct[-16:]))\n",
    "        action = self.get_action_by_idx(old_state, sorted_indices)\n",
    "        return action\n",
    "\n",
    "    def activate(self, gate_idx, x):\n",
    "        if gate_idx == 1:\n",
    "            return x\n",
    "        elif gate_idx == 2:\n",
    "            return np.where(x >= 0, 1, 0)\n",
    "        elif gate_idx == 3:\n",
    "            return np.sin(np.pi * x)\n",
    "        elif gate_idx == 4:\n",
    "            return np.exp(-(x * x) / 2.0)\n",
    "        elif gate_idx == 5:\n",
    "            return np.tanh(x)\n",
    "        elif gate_idx == 6:\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        elif gate_idx == 7:\n",
    "            return -x\n",
    "        elif gate_idx == 8:\n",
    "            return np.abs(x)\n",
    "        elif gate_idx == 9:\n",
    "            return np.max(x, 0)\n",
    "        elif gate_idx == 0:\n",
    "            return np.cos(np.pi * x)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def wan(environment):\n",
    "    # drl = WAN(-1.5)\n",
    "    with open('0.783_test.pickle', 'rb') as handle:\n",
    "        drl = pickle.load(handle)\n",
    "\n",
    "    best_winrate = 0.78\n",
    "    while True:\n",
    "        # Training loop\n",
    "        wan_wins = 0\n",
    "        epochs = 50\n",
    "        while True:\n",
    "            for epoch in range(epochs):\n",
    "                state_old = environment.reset()\n",
    "                turn = 0\n",
    "                while True:\n",
    "                    if not turn % 2:\n",
    "                        action_now = drl.get_action(state_old)\n",
    "                        state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                        if done:\n",
    "                            break\n",
    "                        state_old = state_new\n",
    "                        turn += 1\n",
    "                    else:\n",
    "                        action_now = random.choice((drl.get_possible_actions(state_old)))\n",
    "                        state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                        if done:\n",
    "                            wan_wins += 1\n",
    "                            break\n",
    "                        state_old = state_new\n",
    "                        turn += 1\n",
    "            if wan_wins / epochs > 0.8:\n",
    "                # with open(str(wan_wins/epochs) + '_training.pickle', 'wb') as handle:\n",
    "                #     pickle.dump(drl, handle)\n",
    "                print(f'Winrate of WAN model in training is: {wan_wins / epochs * 100}')\n",
    "                break\n",
    "            else:\n",
    "                drl.mutate(wan_wins / epochs)\n",
    "                drl.tune_weights()\n",
    "                wan_wins = 0\n",
    "\n",
    "        # Test loop\n",
    "        wan_wins = 0\n",
    "        epochs = 1000\n",
    "        for epoch in range(epochs):\n",
    "            state_old = environment.reset()\n",
    "            turn = 0\n",
    "            while True:\n",
    "                if not turn % 2:\n",
    "                    action_now = drl.get_action(state_old)\n",
    "                    state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                    if done:\n",
    "                        break\n",
    "                    state_old = state_new\n",
    "                    turn += 1\n",
    "                else:\n",
    "                    action_now = random.choice((drl.get_possible_actions(state_old)))\n",
    "                    state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                    if done:\n",
    "                        wan_wins += 1\n",
    "                        break\n",
    "                    state_old = state_new\n",
    "                    turn += 1\n",
    "\n",
    "        print(f'Winrate of WAN model in testing is: {wan_wins / epochs * 100}')\n",
    "        print(f'')\n",
    "        if wan_wins / epochs > best_winrate:\n",
    "            with open(str(wan_wins/epochs) + '_test.pickle', 'wb') as handle:\n",
    "                pickle.dump(drl, handle)\n",
    "            # break\n",
    "        else:\n",
    "            drl.mutate(wan_wins / epochs)\n",
    "            drl.tune_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winrate of WAN model in training is: 82.0\n",
      "Winrate of WAN model in testing is: 76.5\n",
      "\n",
      "Winrate of WAN model in training is: 84.0\n",
      "Winrate of WAN model in testing is: 78.2\n",
      "\n",
      "Winrate of WAN model in training is: 88.0\n",
      "Winrate of WAN model in testing is: 78.7\n",
      "\n",
      "Winrate of WAN model in training is: 82.0\n",
      "Winrate of WAN model in testing is: 78.0\n",
      "\n",
      "Winrate of WAN model in training is: 84.0\n",
      "Winrate of WAN model in testing is: 78.2\n",
      "\n",
      "Winrate of WAN model in training is: 86.0\n",
      "Winrate of WAN model in testing is: 77.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [94], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mwan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [91], line 234\u001B[0m, in \u001B[0;36mwan\u001B[1;34m(environment)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m turn \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m--> 234\u001B[0m         action_now \u001B[38;5;241m=\u001B[39m \u001B[43mdrl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_old\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         state_new, reward_now, done, _ \u001B[38;5;241m=\u001B[39m environment\u001B[38;5;241m.\u001B[39mstep(action_now)\n\u001B[0;32m    236\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "Cell \u001B[1;32mIn [91], line 189\u001B[0m, in \u001B[0;36mWAN.get_action\u001B[1;34m(self, old_state)\u001B[0m\n\u001B[0;32m    186\u001B[0m     rawAct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maVec[iNode], rawAct\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    187\u001B[0m     nodeAct[iNode] \u001B[38;5;241m=\u001B[39m rawAct\n\u001B[1;32m--> 189\u001B[0m sorted_indices \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margsort\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodeAct\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_action_by_idx(old_state, sorted_indices)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m action\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36margsort\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\Desktop\\MSI\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1120\u001B[0m, in \u001B[0;36margsort\u001B[1;34m(a, axis, kind, order)\u001B[0m\n\u001B[0;32m   1012\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_argsort_dispatcher)\n\u001B[0;32m   1013\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21margsort\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, kind\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1014\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;124;03m    Returns the indices that would sort an array.\u001B[39;00m\n\u001B[0;32m   1016\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1118\u001B[0m \n\u001B[0;32m   1119\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43margsort\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\MSI\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "wan(Nim())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winrate of WAN model in testing is: 76.63\n"
     ]
    }
   ],
   "source": [
    "with open('0.791_test.pickle', 'rb') as handle:\n",
    "    drl = pickle.load(handle)\n",
    "nim = Nim()\n",
    "wan_wins = 0\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    state_old = nim.reset()\n",
    "    turn = 0\n",
    "    while True:\n",
    "        if not turn % 2:\n",
    "            action_now = drl.get_action(state_old)\n",
    "            state_new, reward_now, done, _ = nim.step(action_now)\n",
    "            if done:\n",
    "                break\n",
    "            state_old = state_new\n",
    "            turn += 1\n",
    "        else:\n",
    "            action_now = random.choice((drl.get_possible_actions(state_old)))\n",
    "            state_new, reward_now, done, _ = nim.step(action_now)\n",
    "            if done:\n",
    "                wan_wins += 1\n",
    "                break\n",
    "            state_old = state_new\n",
    "            turn += 1\n",
    "\n",
    "print(f'Winrate of WAN model in testing is: {wan_wins / epochs * 100}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
