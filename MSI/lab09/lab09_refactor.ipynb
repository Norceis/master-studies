{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Nim:\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = tuple([int((x + 1)) for x in range(0, n_rows * 2, 2)])\n",
    "        self.possible_values_in_rows = []\n",
    "\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx + 1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (tuple(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = copy.deepcopy(self.initial_state)\n",
    "\n",
    "        self.win_states = list()\n",
    "        for idx in range(n_rows):\n",
    "            list_of_zeros = [0] * n_rows\n",
    "            list_of_zeros[idx] = 1\n",
    "            self.win_states.append(tuple(list_of_zeros))\n",
    "        self.win_states = tuple(self.win_states)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        # return self.transition_probs[state][action]\n",
    "        next_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "        return next_state\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if next_state in self.win_states:\n",
    "            reward += 10\n",
    "\n",
    "        elif self.is_terminal(next_state):\n",
    "            reward += -10\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)])\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "            self.is_terminal(self.current_state), None\n",
    "\n",
    "\n",
    "class WAN(object):\n",
    "    def __init__(self, init_shared_weight):\n",
    "        self.input_size = 4\n",
    "        self.neurons = np.random.randint(0, 10, 20)\n",
    "        self.connections = np.random.randint(1, 400, 50)\n",
    "        self.weights = np.random.normal(0, 1, 50)\n",
    "        self.weight_bias = -1.5\n",
    "        n_neurons = len(self.neurons)\n",
    "        self.connections_vector = [0] * (n_neurons * n_neurons)\n",
    "        self.set_weight(init_shared_weight, 0)\n",
    "\n",
    "    def set_weight(self, weight, weight_bias):\n",
    "        n_connections = len(self.connections)\n",
    "        if isinstance(weight, (list, np.ndarray)):\n",
    "            weights = weight\n",
    "        else:\n",
    "            weights = [weight] * n_connections\n",
    "\n",
    "        for idx in range(n_connections):\n",
    "            connection_id = self.connections[idx]\n",
    "            self.connections_vector[connection_id] = weights[idx] + weight_bias\n",
    "\n",
    "    def tune_weights(self):\n",
    "        self.set_weight(self.weights, self.weight_bias)\n",
    "\n",
    "    def mutate(self, winrate):\n",
    "        mut_chance = np.exp(-6 * winrate + 3)\n",
    "\n",
    "        for gate_idx in range(len(self.neurons)):\n",
    "            if np.random.rand() < mut_chance * 0.2:\n",
    "                new_gate = np.random.randint(0, 10)\n",
    "                while self.neurons[gate_idx] == new_gate:\n",
    "                    new_gate = np.random.randint(0, 10)\n",
    "                self.neurons[gate_idx] = new_gate\n",
    "\n",
    "        for connection_idx in range(len(self.connections)):\n",
    "            if np.random.rand() < mut_chance * 0.05:\n",
    "                new_connection = np.random.randint(1, 400)\n",
    "                self.connections[connection_idx] = new_connection\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if state == (0, 0, 0, 0):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_action_by_idx(self, old_state, indices):\n",
    "        move_dict = {0: (1, 0, 0, 0),\n",
    "                     1: (0, 1, 0, 0),\n",
    "                     2: (0, 2, 0, 0),\n",
    "                     3: (0, 3, 0, 0),\n",
    "                     4: (0, 0, 1, 0),\n",
    "                     5: (0, 0, 2, 0),\n",
    "                     6: (0, 0, 3, 0),\n",
    "                     7: (0, 0, 4, 0),\n",
    "                     8: (0, 0, 5, 0),\n",
    "                     9: (0, 0, 0, 1),\n",
    "                     10: (0, 0, 0, 2),\n",
    "                     11: (0, 0, 0, 3),\n",
    "                     12: (0, 0, 0, 4),\n",
    "                     13: (0, 0, 0, 5),\n",
    "                     14: (0, 0, 0, 6),\n",
    "                     15: (0, 0, 0, 7)}\n",
    "\n",
    "        possible_actions = self.get_possible_actions(old_state)\n",
    "        if (0, 0, 0, 0) == possible_actions[0]:\n",
    "            return tuple[0, 0, 0, 0]\n",
    "\n",
    "        for idx in indices:\n",
    "            if move_dict[idx] in possible_actions:\n",
    "                return move_dict[idx]\n",
    "\n",
    "    def get_action(self, old_state):\n",
    "\n",
    "        n_neurons = len(self.neurons)\n",
    "        connection_matrix = np.array(self.connections_vector).reshape((n_neurons, n_neurons))\n",
    "        node_vector = [0] * n_neurons\n",
    "\n",
    "        for i in range(len(old_state)):\n",
    "            node_vector[i] = old_state[i]\n",
    "\n",
    "        for neuron in range(self.input_size, n_neurons):\n",
    "            product = np.dot(node_vector, connection_matrix[:, neuron:neuron + 1])\n",
    "            product = self.activate(self.neurons[neuron], product.tolist()[0])\n",
    "            node_vector[neuron] = product\n",
    "\n",
    "        sorted_indices = np.argsort(-1 * np.array(node_vector[-16:]))\n",
    "        action = self.get_action_by_idx(old_state, sorted_indices)\n",
    "        return action\n",
    "\n",
    "    def activate(self, gate_idx, x):\n",
    "        if gate_idx == 1:\n",
    "            return x\n",
    "        elif gate_idx == 2:\n",
    "            return np.where(x >= 0, 1, 0)\n",
    "        elif gate_idx == 3:\n",
    "            return np.sin(np.pi * x)\n",
    "        elif gate_idx == 4:\n",
    "            return np.exp(-(x * x) / 2.0)\n",
    "        elif gate_idx == 5:\n",
    "            return np.tanh(x)\n",
    "        elif gate_idx == 6:\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        elif gate_idx == 7:\n",
    "            return -x\n",
    "        elif gate_idx == 8:\n",
    "            return np.abs(x)\n",
    "        elif gate_idx == 9:\n",
    "            return np.max(x, 0)\n",
    "        elif gate_idx == 0:\n",
    "            return np.cos(np.pi * x)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def wan(environment):\n",
    "    drl = WAN(-1.5)\n",
    "    # with open('0.761_test.pickle', 'rb') as handle:\n",
    "    #     drl = pickle.load(handle)\n",
    "    #\n",
    "    best_winrate = 0.7\n",
    "    while True:\n",
    "        # Training loop\n",
    "        wan_wins = 0\n",
    "        epochs = 100\n",
    "        while True:\n",
    "            for epoch in range(epochs):\n",
    "                state_old = environment.reset()\n",
    "                turn = 0\n",
    "                while True:\n",
    "                    if not turn % 2:\n",
    "                        action_now = drl.get_action(state_old)\n",
    "                        state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                        if done:\n",
    "                            break\n",
    "                        state_old = state_new\n",
    "                        turn += 1\n",
    "                    else:\n",
    "                        action_now = random.choice((drl.get_possible_actions(state_old)))\n",
    "                        state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                        if done:\n",
    "                            wan_wins += 1\n",
    "                            break\n",
    "                        state_old = state_new\n",
    "                        turn += 1\n",
    "            if wan_wins / epochs > 0.8:\n",
    "                print(f'Winrate of WAN model in training is: {wan_wins / epochs * 100}')\n",
    "                break\n",
    "            else:\n",
    "                drl.mutate(wan_wins / epochs)\n",
    "                drl.tune_weights()\n",
    "                wan_wins = 0\n",
    "\n",
    "        # Test loop\n",
    "        wan_wins = 0\n",
    "        epochs = 1000\n",
    "        for epoch in range(epochs):\n",
    "            state_old = environment.reset()\n",
    "            turn = 0\n",
    "            while True:\n",
    "                if not turn % 2:\n",
    "                    action_now = drl.get_action(state_old)\n",
    "                    state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                    if done:\n",
    "                        break\n",
    "                    state_old = state_new\n",
    "                    turn += 1\n",
    "                else:\n",
    "                    action_now = random.choice((drl.get_possible_actions(state_old)))\n",
    "                    state_new, reward_now, done, _ = environment.step(action_now)\n",
    "                    if done:\n",
    "                        wan_wins += 1\n",
    "                        break\n",
    "                    state_old = state_new\n",
    "                    turn += 1\n",
    "\n",
    "        print(f'Winrate of WAN model in testing is: {wan_wins / epochs * 100}')\n",
    "        print(f'')\n",
    "        if wan_wins / epochs > best_winrate:\n",
    "            with open(str(wan_wins/epochs) + '_test.pickle', 'wb') as handle:\n",
    "                pickle.dump(drl, handle)\n",
    "            # break\n",
    "        else:\n",
    "            drl.mutate(wan_wins / epochs)\n",
    "            drl.tune_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [35], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mwan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [34], line 234\u001B[0m, in \u001B[0;36mwan\u001B[1;34m(environment)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m turn \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m--> 234\u001B[0m         action_now \u001B[38;5;241m=\u001B[39m \u001B[43mdrl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_old\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         state_new, reward_now, done, _ \u001B[38;5;241m=\u001B[39m environment\u001B[38;5;241m.\u001B[39mstep(action_now)\n\u001B[0;32m    236\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "Cell \u001B[1;32mIn [34], line 185\u001B[0m, in \u001B[0;36mWAN.get_action\u001B[1;34m(self, old_state)\u001B[0m\n\u001B[0;32m    182\u001B[0m     node_vector[i] \u001B[38;5;241m=\u001B[39m old_state[i]\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m neuron \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, n_neurons):\n\u001B[1;32m--> 185\u001B[0m     product \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_vector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconnection_matrix\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneuron\u001B[49m\u001B[43m:\u001B[49m\u001B[43mneuron\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m     product \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneurons[neuron], product\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    187\u001B[0m     node_vector[neuron] \u001B[38;5;241m=\u001B[39m product\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "wan(Nim())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winrate of WAN model in testing is: 73.83\n"
     ]
    }
   ],
   "source": [
    "with open('0.761_test.pickle', 'rb') as handle:\n",
    "    drl = pickle.load(handle)\n",
    "nim = Nim()\n",
    "wan_wins = 0\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    state_old = nim.reset()\n",
    "    turn = 0\n",
    "    while True:\n",
    "        if not turn % 2:\n",
    "            action_now = drl.get_action(state_old)\n",
    "            state_new, reward_now, done, _ = nim.step(action_now)\n",
    "            if done:\n",
    "                break\n",
    "            state_old = state_new\n",
    "            turn += 1\n",
    "        else:\n",
    "            action_now = random.choice((drl.get_possible_actions(state_old)))\n",
    "            state_new, reward_now, done, _ = nim.step(action_now)\n",
    "            if done:\n",
    "                wan_wins += 1\n",
    "                break\n",
    "            state_old = state_new\n",
    "            turn += 1\n",
    "\n",
    "print(f'Winrate of WAN model in testing is: {wan_wins / epochs * 100}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
