{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratorium 7\n",
    "\n",
    "Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dołączenie bibliotek do obsługi sieci neuronowych"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1 - Actor-Critic\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n",
    "    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n",
    "    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n",
    "Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n",
    "\\end{equation*}\n",
    "Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n",
    "\\end{equation*}\n",
    "gdzie:\n",
    "\\begin{equation*}\n",
    "    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n",
    "\\end{equation*}\n",
    "</p>"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_size, action_size, actor, critic):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99  # discount rate\n",
    "        self.actor = actor\n",
    "        self.critic = critic  # critic network should have only one output\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.done_memory = []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, basing on policy returned by the network.\n",
    "\n",
    "        Note: To pick action according to the probability generated by the network\n",
    "        \"\"\"\n",
    "\n",
    "        prediction = self.actor.predict_on_batch(state)\n",
    "        best_action = np.random.choice(np.arange(action_size), p=prediction[0])\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_cumulative_rewards(self):\n",
    "        \"\"\"\n",
    "        based on https://github.com/yandexdataschool/Practical_RL/blob/spring20/week06_policy_based/reinforce_tensorflow.ipynb\n",
    "        take a list of immediate rewards r(s,a) for the whole session\n",
    "        compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "        R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "        The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "        and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "\n",
    "        You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "        \"\"\"\n",
    "\n",
    "        n_rewards = len(self.reward_memory)\n",
    "        cumulative_rewards = [0] * n_rewards\n",
    "        cumulative_rewards[-1] = self.reward_memory[-1]\n",
    "        for reward_idx in range(n_rewards - 2, -1, -1):\n",
    "            cumulative_rewards[reward_idx] = self.reward_memory[reward_idx] + self.gamma * cumulative_rewards[reward_idx+1]\n",
    "        return tf.convert_to_tensor(cumulative_rewards, dtype=tf.float32)\n",
    "\n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        self.state_memory.append(state)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "        self.next_state_memory.append(next_state)\n",
    "        self.done_memory.append(done)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Function learn networks using information about state, action, reward and next state.\n",
    "        First the values for state and next_state should be estimated based on output of critic network.\n",
    "        Critic network should be trained based on target value:\n",
    "        target = r + \\gamma next_state_value if not done]\n",
    "        target = r if done.\n",
    "        Actor network shpuld be trained based on delta value:\n",
    "        delta = target - state_value\n",
    "        \"\"\"\n",
    "\n",
    "        for idx in range(len(self.state_memory)):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                actor_state_prediction = self.actor(self.state_memory[idx], training=True)\n",
    "                critic_state_prediction =  self.critic(self.state_memory[idx], training=True)\n",
    "                critic_next_state_prediction = self.critic(self.next_state_memory[idx], training=True)\n",
    "                critic_loss = self.reward_memory[idx] + self.gamma * critic_next_state_prediction * (1 - int(self.done_memory[idx])) - critic_state_prediction\n",
    "                log_prob = tf.math.log(actor_state_prediction[0, self.action_memory[idx]])\n",
    "                actor_loss = -log_prob * critic_loss\n",
    "                critic_loss = critic_loss ** 2\n",
    "\n",
    "            grads_actor = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            grads_critic = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "\n",
    "            self.actor.optimizer.apply_gradients(zip(grads_actor, self.actor.trainable_variables))\n",
    "            self.critic.optimizer.apply_gradients(zip(grads_critic, self.critic.trainable_variables))\n",
    "\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.done_memory = []"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"CartPole-v1\").env\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "alpha_learning_rate = 0.0001\n",
    "beta_learning_rate = 0.0005\n",
    "\n",
    "actor = tf.keras.Sequential()\n",
    "actor.add(tf.keras.layers.Dense(100, input_shape=(state_size,), activation='relu'))\n",
    "actor.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "actor.add(tf.keras.layers.Dense(action_size, activation='softmax'))\n",
    "actor.compile(loss=tf.keras.losses.huber, optimizer=tf.keras.optimizers.Adam(learning_rate=alpha_learning_rate), run_eagerly=True)\n",
    "\n",
    "critic = tf.keras.Sequential()\n",
    "critic.add(tf.keras.layers.Dense(100, input_shape=(state_size,), activation='relu'))\n",
    "critic.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "critic.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "critic.compile(loss=tf.keras.losses.huber, optimizer=tf.keras.optimizers.Adam(learning_rate=beta_learning_rate), run_eagerly=True)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Czas nauczyć agenta gry w środowisku *CartPool*:"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "agent = REINFORCEAgent(state_size, action_size, actor, critic)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    score_history = []\n",
    "\n",
    "    for i in tqdm(range(100)):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()[0]\n",
    "        state = tf.convert_to_tensor(state[np.newaxis, :], dtype=tf.float32)\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = tf.convert_to_tensor(next_state[np.newaxis, :], dtype=tf.float32)\n",
    "            agent.remember(state, next_state, action, reward, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        agent.learn()\n",
    "        score_history.append(score)\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n",
    "\n",
    "    if np.mean(score_history) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]E:\\guzw\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "100%|██████████| 100/100 [02:24<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:30.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [18:13<00:00, 10.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:235.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [50:26<00:00, 30.27s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:655.480\n",
      "You Win!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
